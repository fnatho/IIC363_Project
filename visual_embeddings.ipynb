{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating visual embeddings\n",
        "\n",
        "##Group:\n",
        "- Federico Natho\n",
        "- Felipe Concha\n",
        "- Francisco Madariaga\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "## Vision encoders implemented:\n",
        "- Masked AutoEncoder (MAE)\n",
        "- Contrastive Language-Image Pretraining (CLIP)\n",
        "- Vision-Transformer (ViT)\n",
        "\n",
        "---\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xm7Ikx-DC0Sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cIq4M5pI053",
        "outputId": "c225061e-c3ce-4759-be2f-163f1f4b62e9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/RecSysCuratorNet/images/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZ6c27XKKqiH",
        "outputId": "ab343798-004c-4a48-e3d1-90c48b762640"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1YiKr4FP7crAvtELiLpVx2qQNaFKXB2Y0/RecSysCuratorNet/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os \n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "images = os.listdir()"
      ],
      "metadata": {
        "id": "uVpXW9ZTX_HF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Masked AutoEncoders (MAE) Embeddings"
      ],
      "metadata": {
        "id": "skL5xqAcsh0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://github.com/facebookresearch/mae"
      ],
      "metadata": {
        "id": "054ylwUAtBkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import requests\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# check whether run in Colab\n",
        "if 'google.colab' in sys.modules:\n",
        "    print('Running in Colab.')\n",
        "    !pip3 install timm==0.4.5  # 0.3.2 does not work in Colab\n",
        "    !git clone https://github.com/facebookresearch/mae.git\n",
        "    sys.path.append('./mae')\n",
        "else:\n",
        "    sys.path.append('..')\n",
        "import models_mae"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g25EmvoXskod",
        "outputId": "2d212441-d7af-4b1b-f0ca-5ab2b4e61743"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Colab.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm==0.4.5\n",
            "  Downloading timm-0.4.5-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from timm==0.4.5) (0.14.0+cu116)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.8/dist-packages (from timm==0.4.5) (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.4->timm==0.4.5) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->timm==0.4.5) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->timm==0.4.5) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->timm==0.4.5) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->timm==0.4.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->timm==0.4.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->timm==0.4.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->timm==0.4.5) (2022.9.24)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.4.5\n",
            "Cloning into 'mae'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Total 39 (delta 0), reused 0 (delta 0), pack-reused 39\u001b[K\n",
            "Unpacking objects: 100% (39/39), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Functions retrieved from MAE documentation\n",
        "# Define the utils\n",
        "\n",
        "imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
        "imagenet_std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "def show_image(image, title=''):\n",
        "    # image is [H, W, 3]\n",
        "    assert image.shape[2] == 3\n",
        "    plt.imshow(torch.clip((image * imagenet_std + imagenet_mean) * 255, 0, 255).int())\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.axis('off')\n",
        "    return\n",
        "\n",
        "def prepare_model(chkpt_dir, arch='mae_vit_large_patch16'):\n",
        "    # build model\n",
        "    model = getattr(models_mae, arch)()\n",
        "    # load model\n",
        "    checkpoint = torch.load(chkpt_dir, map_location='cpu')\n",
        "    msg = model.load_state_dict(checkpoint['model'], strict=False)\n",
        "    print(msg)\n",
        "    return model\n",
        "\n",
        "def run_one_image(img, model):\n",
        "    x = torch.tensor(img)\n",
        "\n",
        "    # make it a batch-like\n",
        "    x = x.unsqueeze(dim=0)\n",
        "    x = torch.einsum('nhwc->nchw', x)\n",
        "\n",
        "    # run MAE\n",
        "    loss, y, mask = model(x.float(), mask_ratio=0.75)\n",
        "    y = model.unpatchify(y)\n",
        "    y = torch.einsum('nchw->nhwc', y).detach().cpu()\n",
        "\n",
        "    # visualize the mask\n",
        "    mask = mask.detach()\n",
        "    mask = mask.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 *3)  # (N, H*W, p*p*3)\n",
        "    mask = model.unpatchify(mask)  # 1 is removing, 0 is keeping\n",
        "    mask = torch.einsum('nchw->nhwc', mask).detach().cpu()\n",
        "    \n",
        "    x = torch.einsum('nchw->nhwc', x)\n",
        "\n",
        "    # masked image\n",
        "    im_masked = x * (1 - mask)\n",
        "\n",
        "    # MAE reconstruction pasted with visible patches\n",
        "    im_paste = x * (1 - mask) + y * mask\n",
        "\n",
        "    # make the plt figure larger\n",
        "    plt.rcParams['figure.figsize'] = [24, 24]\n",
        "\n",
        "    plt.subplot(1, 4, 1)\n",
        "    show_image(x[0], \"original\")\n",
        "\n",
        "    plt.subplot(1, 4, 2)\n",
        "    show_image(im_masked[0], \"masked\")\n",
        "\n",
        "    plt.subplot(1, 4, 3)\n",
        "    show_image(y[0], \"reconstruction\")\n",
        "\n",
        "    plt.subplot(1, 4, 4)\n",
        "    show_image(im_paste[0], \"reconstruction + visible\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "AMZ23PBatQFS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download checkpoint if not exist\n",
        "!wget -nc https://dl.fbaipublicfiles.com/mae/visualize/mae_visualize_vit_large.pth\n",
        "\n",
        "chkpt_dir = 'mae_visualize_vit_large.pth'\n",
        "model_mae = prepare_model(chkpt_dir, 'mae_vit_large_patch16').to(device)\n",
        "print('Model loaded.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8OL0IpStfCe",
        "outputId": "cc7083aa-6f07-4215-fc09-af27ce03a012"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘mae_visualize_vit_large.pth’ already there; not retrieving.\n",
            "\n",
            "<All keys matched successfully>\n",
            "Model loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mae_embeddings = list()"
      ],
      "metadata": {
        "id": "ENaTYJdUWjmX"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image in tqdm(images,desc='Calculating MAE embeddings'):\n",
        "  path_to_image = image\n",
        "\n",
        "  img = Image.open(path_to_image)\n",
        "  img = img.convert('RGB') # To ensure the all of the images have 3 channels\n",
        "  img = img.resize((224, 224))\n",
        "  img = np.array(img) / 255.\n",
        "\n",
        "  assert img.shape == (224, 224, 3)\n",
        "\n",
        "  # normalize by ImageNet mean and std\n",
        "  img = img - imagenet_mean\n",
        "  img = img / imagenet_std\n",
        "\n",
        "  ## Formatting the image as a tensor with specified dimensions for forward_encoder() function.\n",
        "  img_processed = torch.tensor(img)\n",
        "  img_processed = img_processed.unsqueeze(dim=0)\n",
        "  img_processed = torch.einsum('nhwc->nchw', img_processed).to(device)\n",
        "\n",
        "  ## forward_encoder -> returns x, mask, ids_restore\n",
        "  ## defining the masking ratio as 0 to only do the forward pass from the encoder.\n",
        "  mae_representation = model_mae.forward_encoder(img_processed.float(),mask_ratio=0)\n",
        "  embeddings = mae_representation[0].detach()[0].cpu().numpy()\n",
        "  mae_embeddings.append(np.mean(embeddings,axis=0))\n",
        "\n"
      ],
      "metadata": {
        "id": "Fg71oY9BtSxB"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the embeddings as .npy file"
      ],
      "metadata": {
        "id": "HJQ8M2TeXwb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('mae_embeddings_list.npy', 'wb') as f:\n",
        "    np.save(f, mae_embeddings)"
      ],
      "metadata": {
        "id": "96pCgrkaXwb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the files a pickle"
      ],
      "metadata": {
        "id": "cS-YugziXwb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# open a file, where you ant to store the data\n",
        "file = open('mae_embeddings_pickle', 'wb')\n",
        "pickle.dump(mae_embeddings, file)\n",
        "file.close()\n"
      ],
      "metadata": {
        "id": "FqUap0WsXwb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP Embeddings"
      ],
      "metadata": {
        "id": "tA6r4PyPQuU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "References: https://github.com/openai/CLIP"
      ],
      "metadata": {
        "id": "hkRGXDxDFfnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLI96GjFRvVw",
        "outputId": "e1b0fa3e-3e1f-45a9-83de-4c16198baa1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-we5st4w3\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-we5st4w3\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (0.14.0+cu116)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->clip==1.0) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369408 sha256=b998acfc465bc879d1ed332da8fd6e84ac7cf85a79490fe5aee10be32ee7903b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1jl3pcpb/wheels/ab/4f/3a/5e51521b55997aa6f0690e095c08824219753128ce8d9969a3\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torch\n",
        "import clip\n",
        "import pickle"
      ],
      "metadata": {
        "id": "TuTFugRPS-I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, preprocess = clip.load(\"RN50x64\", device=device)\n",
        "embeddings_clip = list()"
      ],
      "metadata": {
        "id": "1P9SlFRBfqBt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a83fee2c-1f1b-4a6b-f1ec-8883288cd129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.26G/1.26G [00:42<00:00, 31.7MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for image in tqdm(images,desc='Encoding images'):\n",
        "  path_to_image = image\n",
        "\n",
        "  ## Reading image\n",
        "  image_processed = preprocess(Image.open(path_to_image)).unsqueeze(0).to(device)\n",
        "\n",
        "  ## Visual-encoding the image\n",
        "  with torch.no_grad():\n",
        "      image_features = model.encode_image(image_processed)\n",
        "\n",
        "  embeddings_clip.append([image,image_features.cpu().tolist()[0]])"
      ],
      "metadata": {
        "id": "xkKGsiurKSI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the embeddings as .npy file"
      ],
      "metadata": {
        "id": "wa-p7bbWSK7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('clip_embeddings_list.npy', 'wb') as f:\n",
        "    np.save(f, embeddings_clip)"
      ],
      "metadata": {
        "id": "OYd1VJ48Q3rD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the files a pickle"
      ],
      "metadata": {
        "id": "kxeLE5FXSM9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# open a file, where you ant to store the data\n",
        "file = open('clip_embeddings_pickle', 'wb')\n",
        "pickle.dump(embeddings_clip, file)\n",
        "file.close()\n"
      ],
      "metadata": {
        "id": "_yQL12aXRAhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT Embeddings"
      ],
      "metadata": {
        "id": "G31VmB_pOoL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTQO5y2NECoa",
        "outputId": "3dfd977e-c5fe-476e-b753-9a4b40caee93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 61.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 79.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTConfig, ViTModel, ViTFeatureExtractor\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "\n",
        "model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\").to(device)\n"
      ],
      "metadata": {
        "id": "zeYZ8oLlEjMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "featureExtractor = ViTFeatureExtractor()\n",
        "embedding_vit=[]\n",
        "\n",
        "for image in tqdm(images):\n",
        "    images_n= Image.open(image)\n",
        "    images_n = images_n.convert('RGB')\n",
        "    inputs = featureExtractor(images_n, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    last_hidden_states = outputs.last_hidden_state.cpu().numpy()\n",
        "\n",
        "    embedding = np.mean(np.squeeze(last_hidden_states, axis=0),axis=0)\n",
        "    embedding_vit.append([image,embedding])\n",
        "    del images_n, inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byEdijTsEJ1o",
        "outputId": "e6e561f2-71e6-45bb-dfa6-49da7c37bd65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13297/13297 [1:03:40<00:00,  3.48it/s]\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = np.asanyarray(arr)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the embeddings as .npy file"
      ],
      "metadata": {
        "id": "HVN5ThrUP3Pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/drive/MyDrive/embedding/embeddings_vit_2.npy', embedding_vit, allow_pickle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5F6w1KjHDOg",
        "outputId": "66037bf6-bfc3-46b7-c8ca-b93b79ec0c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = np.asanyarray(arr)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the embeddings as pickle file"
      ],
      "metadata": {
        "id": "K3_N8ONsP5Tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filehandler = open(\"embedding_vit_pickle\",\"rb\")\n",
        "a = pickle.load(filehandler)\n",
        "filehandler.close()"
      ],
      "metadata": {
        "id": "RlBYNcYxSPu7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "skL5xqAcsh0r",
        "tA6r4PyPQuU5",
        "G31VmB_pOoL6"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}